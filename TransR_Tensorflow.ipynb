{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三元组数量 483142\n",
      "实体和关系数量 16296\n"
     ]
    }
   ],
   "source": [
    "f = open('链路预测数据集/FB15k/freebase_mtr100_mte100-train.txt')\n",
    "data = f.read()\n",
    "triples = data.split('\\n')\n",
    "triples = triples[:483142]\n",
    "\n",
    "totals = list([entity_and_relation for triple in triples for entity_and_relation in triple.split('\\t')])\n",
    "print('三元组数量',len(triples))\n",
    "print('实体和关系数量',len(set(totals)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实体数 14951\n",
      "关系数 1345\n"
     ]
    }
   ],
   "source": [
    "head_entities = list([triple.split('\\t')[0] for triple in triples])\n",
    "relations =  list([triple.split('\\t')[1] for triple in triples])\n",
    "tail_entities =  list([triple.split('\\t')[2] for triple in triples])\n",
    "\n",
    "# 全部实体\n",
    "total_entities = head_entities + tail_entities\n",
    "\n",
    "print('实体数',len(set(head_entities + tail_entities)))\n",
    "print('关系数',len(set(relations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 负采样三元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 生成负采样的三元组\n",
    "invalid_head_entities = []\n",
    "invalid_relations = []\n",
    "invalid_tail_entities = []\n",
    "\n",
    "for i in range(len(relations)):\n",
    "    random_int = random.randint(0,len(head_entities)-1)\n",
    "    random_int1 = random.randint(0,len(head_entities)-1)\n",
    "    \n",
    "    # 不同时替换头尾实体\n",
    "    if i%2 ==0:\n",
    "        invalid_head_entities.append(head_entities[i])\n",
    "        invalid_relations.append(relations[i])\n",
    "        invalid_tail_entities.append(tail_entities[random_int1])\n",
    "    else:\n",
    "        invalid_head_entities.append(head_entities[random_int])\n",
    "        invalid_relations.append(relations[i])\n",
    "        invalid_tail_entities.append(tail_entities[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实体和关系编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实体索引长度 14951\n",
      "关系索引长度 1345\n",
      "934 91 7744\n",
      "2872 120 10893\n",
      "3642 861 8034\n",
      "934 91 2159\n",
      "12175 120 10893\n",
      "3642 861 8948\n"
     ]
    }
   ],
   "source": [
    "# 为每个实体/关系分配倒排索引\n",
    "# 根据 名称 查找 id\n",
    "reverse_index_entities = dict([key,index] for index,key in enumerate(set(total_entities)))\n",
    "print('实体索引长度',len(reverse_index_entities))\n",
    "\n",
    "reverse_index_relation = dict([key,index] for index,key in enumerate(set(relations)))\n",
    "print('关系索引长度',len(reverse_index_relation))\n",
    "\n",
    "# 黄金三元组索引\n",
    "head_entities_encode = list([reverse_index_entities[entry] for entry in head_entities])\n",
    "relations_encode = list([reverse_index_relation[relation] for relation in relations])\n",
    "tail_entities_encode = list([reverse_index_entities[entry] for entry in tail_entities])\n",
    "\n",
    "#无效三元组索引\n",
    "invalid_head_entities_encode = list([reverse_index_entities[entry] for entry in invalid_head_entities])\n",
    "invalid_relations_encode = list([reverse_index_relation[relation] for relation in invalid_relations])\n",
    "invalid_tail_entities_encode = list([reverse_index_entities[entry] for entry in invalid_tail_entities])\n",
    "\n",
    "print(head_entities_encode[123456],relations_encode[123456],tail_entities_encode[123456])\n",
    "print(head_entities_encode[12345],relations_encode[12345],tail_entities_encode[12345])\n",
    "print(head_entities_encode[12234],relations_encode[12234],tail_entities_encode[12234])\n",
    "\n",
    "print(invalid_head_entities_encode[123456],invalid_relations_encode[123456],invalid_tail_entities_encode[123456])\n",
    "print(invalid_head_entities_encode[12345],invalid_relations_encode[12345],invalid_tail_entities_encode[12345])\n",
    "print(invalid_head_entities_encode[12234],invalid_relations_encode[12234],invalid_tail_entities_encode[12234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 组合数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(934, 91, 7744)\n",
      "(934, 91, 2159)\n",
      "三元组数量 483142\n",
      "三元组数量 483142\n"
     ]
    }
   ],
   "source": [
    "# 编码后的数据\n",
    "golden_data = list(zip(head_entities_encode,relations_encode,tail_entities_encode))\n",
    "invalid_data = list(zip(invalid_head_entities_encode,invalid_relations_encode,invalid_tail_entities_encode))\n",
    "    \n",
    "print(golden_data[123456])\n",
    "print(invalid_data[123456])\n",
    "\n",
    "print('三元组数量',len(golden_data))\n",
    "print('三元组数量',len(invalid_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = 0 \n",
    "def nextbatch(batchsize=400):\n",
    "    global keep\n",
    "    start = keep\n",
    "    end = start + batchsize\n",
    "    \n",
    "    if end > len(golden_data):\n",
    "        end = end % len(golden_data)\n",
    "        keep = end\n",
    "        return golden_data[start:]+golden_data[:end],invalid_data[start:]+invalid_data[:end]\n",
    "    else:\n",
    "        keep = end\n",
    "        return golden_data[start:end],invalid_data[start:end]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "# 实体数量\n",
    "entities_num = len(reverse_index_entities)\n",
    "# 关系数量\n",
    "relations_num = len(reverse_index_relation)\n",
    "dim = 100\n",
    "margin = 1.50\n",
    "learning_rate = 1e-2\n",
    "\n",
    "# embedding层\n",
    "ent_embeds =  tf.Variable(tf.truncated_normal([entities_num, dim], stddev=1.0 / math.sqrt(dim)))\n",
    "\n",
    "rel_embeds =  tf.Variable(tf.truncated_normal([relations_num, dim], stddev=1.0 / math.sqrt(dim)))\n",
    "\n",
    "rel_matrix =  tf.Variable(tf.truncated_normal([relations_num, dim * dim], stddev=1.0 / math.sqrt(dim)))\n",
    "\n",
    "#定义输入\n",
    "pos_hs = tf.placeholder(tf.int32, shape=[None])\n",
    "pos_rs = tf.placeholder(tf.int32, shape=[None])\n",
    "pos_ts = tf.placeholder(tf.int32, shape=[None])\n",
    "neg_hs = tf.placeholder(tf.int32, shape=[None])\n",
    "neg_rs = tf.placeholder(tf.int32, shape=[None])\n",
    "neg_ts = tf.placeholder(tf.int32, shape=[None])\n",
    "\n",
    "#从embedding层取值\n",
    "# 实体向量需要转置，这里用reshape\n",
    "phs_origin = tf.reshape(tf.nn.embedding_lookup(ent_embeds, pos_hs), [-1, dim, 1])\n",
    "prs = tf.nn.embedding_lookup(rel_embeds, pos_rs)\n",
    "pts_origin = tf.reshape(tf.nn.embedding_lookup(ent_embeds, pos_ts), [-1, dim, 1])\n",
    "\n",
    "nhs_origin = tf.reshape(tf.nn.embedding_lookup(ent_embeds, neg_hs), [-1, dim, 1])\n",
    "nrs = tf.nn.embedding_lookup(rel_embeds, neg_rs)\n",
    "nts_origin = tf.reshape(tf.nn.embedding_lookup(ent_embeds, neg_ts), [-1, dim, 1])\n",
    "\n",
    "p_matrix = tf.reshape(tf.nn.embedding_lookup(rel_matrix, pos_rs),\n",
    "                                  [-1, dim, dim])\n",
    "n_matrix = tf.reshape(tf.nn.embedding_lookup(rel_matrix, neg_rs),\n",
    "                                  [-1, dim, dim])\n",
    "\n",
    "phs = tf.reshape(tf.matmul(p_matrix, phs_origin), [-1, dim])\n",
    "pts = tf.reshape(tf.matmul(p_matrix, pts_origin), [-1, dim])\n",
    "phs = tf.nn.l2_normalize(phs, 1)\n",
    "pts = tf.nn.l2_normalize(pts, 1)\n",
    "\n",
    "nhs = tf.reshape(tf.matmul(n_matrix, nhs_origin), [-1, dim])\n",
    "nts = tf.reshape(tf.matmul(n_matrix, nts_origin), [-1, dim])\n",
    "nhs = tf.nn.l2_normalize(nhs, 1)\n",
    "nts = tf.nn.l2_normalize(nts, 1)\n",
    "\n",
    "# margin loss\n",
    "pos_triple_loss = tf.reduce_sum(tf.abs(phs + prs - pts),axis = 1)\n",
    "neg_triple_loss = tf.reduce_sum(tf.abs(nhs + nrs - nts),axis = 1)\n",
    "triple_loss = tf.reduce_sum(tf.nn.relu(tf.constant(margin) + pos_triple_loss - neg_triple_loss))\n",
    "\n",
    "# limited_loss\n",
    "# pos_margin = 0.50\n",
    "# neg_margin = 5.50\n",
    "# balance =  0.2\n",
    "# pos_score = tf.reduce_sum(tf.square(phs + prs - pts),axis = 1)\n",
    "# neg_score = tf.reduce_sum(tf.square(nhs + nrs - nts),axis = 1)\n",
    "# pos_loss = tf.reduce_sum(tf.nn.relu(pos_score - tf.constant(pos_margin)))\n",
    "# neg_loss = tf.reduce_sum(tf.nn.relu(tf.constant(neg_margin) - neg_score))\n",
    "# triple_loss = tf.add(pos_loss, balance * neg_loss, name='limited_loss')\n",
    "        \n",
    "triple_optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(triple_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JDUSER\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "epochs_num = 200\n",
    "batch_size = 2000\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 0\n",
      "epoch 0, >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>,loss_val 869.51178000\n",
      "step 0, loss_val 238394.457062\n",
      "time cost 30.717217922210693 s\n",
      "start: 858\n",
      "epoch 1, >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>,loss_val 118.2953644\n",
      "step 1, loss_val 66248.383385\n",
      "time cost 33.66998028755188 s\n",
      "start: 1716\n",
      "epoch 2, >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>,loss_val 42.50317882\n",
      "step 2, loss_val 31592.714468\n",
      "time cost 32.818262338638306 s\n",
      "start: 2574\n",
      "epoch 3, >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>,loss_val 26.55599664\n",
      "step 3, loss_val 18066.700434\n",
      "time cost 31.113813161849976 s\n",
      "start: 3432\n",
      " epoch 4, >>>>>>>>>>>>>>>>                                            ,loss_val 93.8384021"
     ]
    }
   ],
   "source": [
    "for step in range(epochs_num):\n",
    "    time_start = time.time() #开始计时\n",
    "    loss_total = 0\n",
    "    print('start:',keep)\n",
    "    for i in range(len(golden_data)//batch_size + 1):\n",
    "#         print('batch_size',batch_size)\n",
    "#         print('range',range(len(golden_data)//batch_size + 1))\n",
    "        batch_pos,batch_neg = nextbatch(batchsize=batch_size)   \n",
    "        feed_dict = {pos_hs: [x[0] for x in batch_pos], pos_rs: [x[1] for x in batch_pos], pos_ts: [x[2] for x in batch_pos],\n",
    "                     neg_hs: [x[0] for x in batch_neg], neg_rs: [x[1] for x in batch_neg], neg_ts: [x[2] for x in batch_neg]}\n",
    "        loss_val, _ = sess.run([triple_loss, triple_optimizer], feed_dict=feed_dict)\n",
    "        # 损失统计\n",
    "        loss_total += loss_val\n",
    "        data_pass = int(i*60/(len(golden_data)//batch_size + 1)) + 1\n",
    "        data_left = 60 - data_pass\n",
    "        \n",
    "        print(\"\\r epoch %d, %s%s,loss_val %f\" % (step,data_pass*'>',data_left*' ',loss_val),end='')\n",
    "    print(\"\\repoch %d, %s%s,loss_val %f\" % (step,data_pass*'>',data_left*' ',loss_val),end='\\n')\n",
    "    print(\"\\rstep %d, loss_val %f\" % (step, loss_total/(len(batch_pos)//batch_size + 1)))\n",
    "    time_end = time.time()    #结束计时\n",
    "    sum_t= time_end - time_start   #运行所花时间\n",
    "    print('time cost', sum_t, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()    # 生成saver\n",
    "saver.save(sess, \"./tensorflow_transr_model.h5\")     # 当路径不存在时，会自动创建路径"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pos,batch_neg = nextbatch(batchsize=batch_size)   \n",
    "feed_dict = {pos_hs: [x[0] for x in batch_pos], pos_rs: [x[1] for x in batch_pos], pos_ts: [x[2] for x in batch_pos],\n",
    "                     neg_hs: [x[0] for x in batch_neg], neg_rs: [x[1] for x in batch_neg], neg_ts: [x[2] for x in batch_neg]}\n",
    "\n",
    "print(sess.run(pos_triple_loss, feed_dict=feed_dict))\n",
    "print(sess.run(neg_triple_loss, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_embedding_index = []\n",
    "rel_embedding_index = []\n",
    "rel_mapping_index = []\n",
    "\n",
    "#为关系 属性构建索引\n",
    "\n",
    "entity_ids = [reverse_index_entities[i] for i in list(reverse_index_entities)]\n",
    "relation_ids = [reverse_index_relation[i] for i in list(reverse_index_relation)]\n",
    "\n",
    "entities_embedding = sess.run(phs_origin,feed_dict={pos_hs:entity_ids})\n",
    "relation_embedding = sess.run(prs,feed_dict={pos_rs:relation_ids})\n",
    "relation_matrix = sess.run(p_matrix,feed_dict={pos_rs:relation_ids})\n",
    "\n",
    "# url到embedding的三个字典\n",
    "entities_embedding_dict = dict(zip(list(reverse_index_entities),entities_embedding))\n",
    "relation_embedding_dict = dict(zip(list(reverse_index_relation),relation_embedding))\n",
    "relation_matrix_dict = dict(zip(list(reverse_index_relation),relation_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入训练数据\n",
    "# 先用hit@10标准测试一下\n",
    "valid_file = open('链路预测数据集/FB15k/freebase_mtr100_mte100-valid.txt')\n",
    "valid_data = valid_file.read()\n",
    "valid_triples = valid_data.split('\\n')[:50000]\n",
    "\n",
    "#生成验证集实体和关系集合\n",
    "valid_head_entities = list([triple.split('\\t')[0] for triple in valid_triples])\n",
    "valid_relations =  list([triple.split('\\t')[1] for triple in valid_triples])\n",
    "valid_tail_entities =  list([triple.split('\\t')[2] for triple in valid_triples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "链接预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('开始预测')\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "hit10 = 0\n",
    "hit50 = 0\n",
    "hit100 = 0\n",
    "num = 0\n",
    "\n",
    "def takeFirst(elem):\n",
    "    return elem[0]\n",
    "\n",
    "for head,relation,tail in zip(valid_head_entities[:1000],valid_relations[:1000],valid_tail_entities[:1000]):\n",
    "    num = num+1\n",
    "    time_start = time.time() #开始计时\n",
    "\n",
    "    print(head,relation,tail)\n",
    "    #　头实体嵌入\n",
    "    valid_head_embedding = entities_embedding_dict[head]\n",
    "    #　关系的嵌入\n",
    "    valid_relation_embedding = relation_embedding_dict[relation]\n",
    "    # 关系的投影\n",
    "    valid_relation_matrix = relation_matrix_dict[relation]\n",
    "    \n",
    "    \n",
    "    mapped_head = np.dot(valid_relation_matrix,valid_head_embedding).T\n",
    "    mapped_head_and_relation = mapped_head + valid_relation_embedding\n",
    "\n",
    "    #　与所有的实体计算距离\n",
    "    distance_list = []\n",
    "    for index,entity in enumerate(list(reverse_index_entities)):\n",
    "        tail_embedding = entities_embedding_dict[entity]\n",
    "        mapped_tail_embedding = np.dot(valid_relation_matrix,tail_embedding).T\n",
    "        # 二阶距离\n",
    "        distance = np.linalg.norm(mapped_head_and_relation - mapped_tail_embedding)\n",
    "        distance_list.append([distance,entity])\n",
    "\n",
    "    # 对所有距离进行排序\n",
    "    distance_list.sort(key=takeFirst)\n",
    "    \n",
    "    predict_100_list = list([entity[1] for entity in distance_list])[:100]\n",
    "    predict_50_list = list([entity[1] for entity in distance_list])[:50]\n",
    "    predict_10_list = list([entity[1] for entity in distance_list])[:10]\n",
    "\n",
    "    if tail in predict_100_list:\n",
    "        hit100 = hit100+1\n",
    "        \n",
    "    if tail in predict_50_list:\n",
    "        hit50 = hit50+1\n",
    "        \n",
    "    if tail in predict_10_list:\n",
    "        hit10 = hit10 + 1\n",
    "\n",
    "    print('hit',hit10/num,hit50/num,hit100/num,num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
