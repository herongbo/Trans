{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 读取训练文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "三元组数量 483142\n",
      "实体总数 16296\n"
     ]
    }
   ],
   "source": [
    "f = open('链路预测数据集/FB15k/freebase_mtr100_mte100-train.txt')\n",
    "data = f.read()\n",
    "triples = data.split('\\n')\n",
    "triples = triples[:483142]\n",
    "\n",
    "totals = list([entity_and_relation for triple in triples for entity_and_relation in triple.split('\\t')])\n",
    "print('三元组数量',len(triples))\n",
    "print('实体总数',len(set(totals)))\n",
    "#嵌入的总数为16296"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成头实体集合 尾实体集合 关系集合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "实体数 14951\n",
      "关系数 1345\n"
     ]
    }
   ],
   "source": [
    "head_entities = list([triple.split('\\t')[0] for triple in triples])\n",
    "relations =  list([triple.split('\\t')[1] for triple in triples])\n",
    "tail_entities =  list([triple.split('\\t')[2] for triple in triples])\n",
    "\n",
    "total_entities = head_entities + tail_entities\n",
    "\n",
    "print('实体数',len(set(head_entities + tail_entities)))\n",
    "print('关系数',len(set(relations)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成负采样三元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/m/02w2bc /education/educational_institution/students_graduates./education/education/major_field_of_study /m/0g26h\n",
      "/m/0kpzy /location/us_county/county_seat /m/0dc95\n",
      "/m/081pw /military/military_conflict/combatants./military/military_combatant_group/combatants /m/0193qj\n",
      "/m/02w2bc /education/educational_institution/students_graduates./education/education/major_field_of_study /m/0sxkh\n",
      "/m/016mhd /location/us_county/county_seat /m/0dc95\n",
      "/m/081pw /military/military_conflict/combatants./military/military_combatant_group/combatants /m/09nqf\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# 生成负采样的三元组\n",
    "invalid_head_entities = []\n",
    "invalid_relations = []\n",
    "invalid_tail_entities = []\n",
    "\n",
    "for i in range(len(relations)):\n",
    "    random_int = random.randint(0,len(head_entities)-1)\n",
    "    random_int1 = random.randint(0,len(head_entities)-1)\n",
    "    \n",
    "    # 不同时替换头尾实体\n",
    "    if i%2 ==0:\n",
    "        invalid_head_entities.append(head_entities[i])\n",
    "        invalid_relations.append(relations[i])\n",
    "        invalid_tail_entities.append(tail_entities[random_int1])\n",
    "    else:\n",
    "        invalid_head_entities.append(head_entities[random_int])\n",
    "        invalid_relations.append(relations[i])\n",
    "        invalid_tail_entities.append(tail_entities[i])\n",
    "#     invalid_head_entities.append(head_entities[random_int])\n",
    "#     invalid_relations.append(relation)\n",
    "#     invalid_tail_entities.append(tail_entities[random_int1])\n",
    "\n",
    "print(head_entities[123456],relations[123456],tail_entities[123456])\n",
    "print(head_entities[12345],relations[12345],tail_entities[12345])\n",
    "print(head_entities[12234],relations[12234],tail_entities[12234])\n",
    "\n",
    "print(invalid_head_entities[123456],invalid_relations[123456],invalid_tail_entities[123456])\n",
    "print(invalid_head_entities[12345],invalid_relations[12345],invalid_tail_entities[12345])\n",
    "print(invalid_head_entities[12234],invalid_relations[12234],invalid_tail_entities[12234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对实体编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "索引长度 16296\n",
      "5633 9134 12907\n",
      "537 11752 4882\n",
      "12520 4639 12228\n",
      "5633 9134 10677\n",
      "10621 11752 4882\n",
      "12520 4639 1595\n"
     ]
    }
   ],
   "source": [
    "# 为每个实体/关系分配倒排索引\n",
    "# 根据 名称 查找 id\n",
    "reverse_index = dict([key,index] for index,key in enumerate(set(totals)))\n",
    "print('索引长度',len(reverse_index))\n",
    "\n",
    "#数据标签化\n",
    "\n",
    "head_entities_encode = list([reverse_index[entry] for entry in head_entities])\n",
    "relations_encode = list([reverse_index[relation] for relation in relations])\n",
    "tail_entities_encode = list([reverse_index[entry] for entry in tail_entities])\n",
    "\n",
    "invalid_head_entities_encode = list([reverse_index[entry] for entry in invalid_head_entities])\n",
    "invalid_relations_encode = list([reverse_index[relation] for relation in invalid_relations])\n",
    "invalid_tail_entities_encode = list([reverse_index[entry] for entry in invalid_tail_entities])\n",
    "\n",
    "print(head_entities_encode[123456],relations_encode[123456],tail_entities_encode[123456])\n",
    "print(head_entities_encode[12345],relations_encode[12345],tail_entities_encode[12345])\n",
    "print(head_entities_encode[12234],relations_encode[12234],tail_entities_encode[12234])\n",
    "\n",
    "print(invalid_head_entities_encode[123456],invalid_relations_encode[123456],invalid_tail_entities_encode[123456])\n",
    "print(invalid_head_entities_encode[12345],invalid_relations_encode[12345],invalid_tail_entities_encode[12345])\n",
    "print(invalid_head_entities_encode[12234],invalid_relations_encode[12234],invalid_tail_entities_encode[12234])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "组合数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5633, 9134, 12907)\n",
      "(5633, 9134, 10677)\n",
      "三元组数量 483142\n",
      "三元组数量 483142\n"
     ]
    }
   ],
   "source": [
    "# 编码后的数据\n",
    "golden_data = list(zip(head_entities_encode,relations_encode,tail_entities_encode))\n",
    "invalid_data = list(zip(invalid_head_entities_encode,invalid_relations_encode,invalid_tail_entities_encode))\n",
    "    \n",
    "print(golden_data[123456])\n",
    "print(invalid_data[123456])\n",
    "\n",
    "print('三元组数量',len(golden_data))\n",
    "print('三元组数量',len(invalid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = 0 \n",
    "def nextbatch(batchsize=400):\n",
    "    global keep\n",
    "    start = keep\n",
    "    end = start + batchsize\n",
    "    if end > len(golden_data):\n",
    "        end = end % len(golden_data)\n",
    "        keep = end\n",
    "        return golden_data[start:]+golden_data[:end],invalid_data[start:]+invalid_data[:end]\n",
    "    else:\n",
    "        keep = end\n",
    "        return golden_data[start:end],invalid_data[start:end]\n",
    "    \n",
    "# golden_data = golden_data[:10]\n",
    "# invalid_data = invalid_data[:10]  \n",
    "\n",
    "# print(nextbatch(3)[0])\n",
    "# print(nextbatch(5)[0])\n",
    "# print(nextbatch(4)[0])\n",
    "# print(nextbatch(4)[0])\n",
    "# print(nextbatch(4)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\JDUSER\\Anaconda3\\envs\\deeplearning\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:189: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "start: 0\n",
      "step 0, loss_val 185.383485\n",
      "time cost 2.8773000240325928 s\n",
      "start: 63\n",
      "step 1, loss_val 23.419429\n",
      "time cost 2.574111223220825 s\n",
      "start: 126\n",
      "step 2, loss_val 6.805881\n",
      "time cost 2.6130075454711914 s\n",
      "start: 189\n",
      "step 3, loss_val 2.649368\n",
      "time cost 3.07078218460083 s\n",
      "start: 252\n",
      "step 4, loss_val 1.293642\n",
      "time cost 3.121647357940674 s\n",
      "start: 315\n",
      "step 5, loss_val 0.752976\n",
      "time cost 2.5950543880462646 s\n",
      "start: 378\n",
      "step 6, loss_val 0.525403\n",
      "time cost 3.6512303352355957 s\n",
      "start: 441\n",
      "step 7, loss_val 0.426328\n",
      "time cost 3.451761484146118 s\n",
      "start: 504\n",
      "step 8, loss_val 0.370923\n",
      "time cost 3.046847105026245 s\n",
      "start: 567\n",
      "step 9, loss_val 0.346636\n",
      "time cost 2.748673915863037 s\n",
      "start: 630\n",
      "step 10, loss_val 0.325319\n",
      "time cost 2.9361133575439453 s\n",
      "start: 693\n",
      "step 11, loss_val 0.315719\n",
      "time cost 3.086737871170044 s\n",
      "start: 756\n",
      "step 12, loss_val 0.305632\n",
      "time cost 3.0418596267700195 s\n",
      "start: 819\n",
      "step 13, loss_val 0.302563\n",
      "time cost 3.441818952560425 s\n",
      "start: 882\n",
      "step 14, loss_val 0.301128\n",
      "time cost 3.5126025676727295 s\n",
      "start: 945\n",
      "step 15, loss_val 0.296857\n",
      "time cost 3.1226439476013184 s\n",
      "start: 1008\n",
      "step 16, loss_val 0.293689\n",
      "time cost 3.019918203353882 s\n",
      "start: 1071\n",
      "step 17, loss_val 0.292904\n",
      "time cost 3.44777512550354 s\n",
      "start: 1134\n",
      "step 18, loss_val 0.291032\n",
      "time cost 3.868678331375122 s\n",
      "start: 1197\n",
      "step 19, loss_val 0.289858\n",
      "time cost 3.6352710723876953 s\n",
      "start: 1260\n",
      "step 20, loss_val 0.287520\n",
      "time cost 3.0468153953552246 s\n",
      "start: 1323\n",
      "step 21, loss_val 0.286874\n",
      "time cost 3.065824031829834 s\n",
      "start: 1386\n",
      "step 22, loss_val 0.285336\n",
      "time cost 3.0468170642852783 s\n",
      "start: 1449\n",
      "step 23, loss_val 0.285591\n",
      "time cost 3.0548269748687744 s\n",
      "start: 1512\n",
      "step 24, loss_val 0.284068\n",
      "time cost 3.6871869564056396 s\n",
      "start: 1575\n",
      "step 25, loss_val 0.283623\n",
      "time cost 3.089677572250366 s\n",
      "start: 1638\n",
      "step 26, loss_val 0.283421\n",
      "time cost 3.0358753204345703 s\n",
      "start: 1701\n",
      "step 27, loss_val 0.283173\n",
      "time cost 3.0119402408599854 s\n",
      "start: 1764\n",
      "step 28, loss_val 0.283416\n",
      "time cost 3.0368716716766357 s\n",
      "start: 1827\n",
      "step 29, loss_val 0.283053\n",
      "time cost 3.0379018783569336 s\n",
      "start: 1890\n",
      "step 30, loss_val 0.282988\n",
      "time cost 3.1007070541381836 s\n",
      "start: 1953\n",
      "step 31, loss_val 0.282988\n",
      "time cost 3.0239033699035645 s\n",
      "start: 2016\n",
      "step 32, loss_val 0.282988\n",
      "time cost 3.119621992111206 s\n",
      "start: 2079\n",
      "step 33, loss_val 0.282988\n",
      "time cost 3.0309207439422607 s\n",
      "start: 2142\n",
      "step 34, loss_val 0.282988\n",
      "time cost 3.0208816528320312 s\n",
      "start: 2205\n",
      "step 35, loss_val 0.282988\n",
      "time cost 3.045849084854126 s\n",
      "start: 2268\n",
      "step 36, loss_val 0.282988\n",
      "time cost 3.031886577606201 s\n",
      "start: 2331\n",
      "step 37, loss_val 0.282988\n",
      "time cost 3.023939609527588 s\n",
      "start: 2394\n",
      "step 38, loss_val 0.282988\n",
      "time cost 3.0458483695983887 s\n",
      "start: 2457\n",
      "step 39, loss_val 0.282988\n",
      "time cost 3.0946860313415527 s\n",
      "start: 2520\n",
      "step 40, loss_val 0.282988\n",
      "time cost 2.842419385910034 s\n",
      "start: 2583\n",
      "step 41, loss_val 0.282988\n",
      "time cost 2.5900683403015137 s\n",
      "start: 2646\n",
      "step 42, loss_val 0.282988\n",
      "time cost 2.579072952270508 s\n",
      "start: 2709\n",
      "step 43, loss_val 0.282988\n",
      "time cost 2.532247304916382 s\n",
      "start: 2772\n",
      "step 44, loss_val 0.282988\n",
      "time cost 2.5880494117736816 s\n",
      "start: 2835\n",
      "step 45, loss_val 0.282988\n",
      "time cost 2.572117567062378 s\n",
      "start: 2898\n",
      "step 46, loss_val 0.282988\n",
      "time cost 2.544215679168701 s\n",
      "start: 2961\n",
      "step 47, loss_val 0.282988\n",
      "time cost 2.5332207679748535 s\n",
      "start: 3024\n",
      "step 48, loss_val 0.282988\n",
      "time cost 2.6089959144592285 s\n",
      "start: 3087\n",
      "step 49, loss_val 0.282988\n",
      "time cost 2.7725794315338135 s\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "\n",
    "vocabulary_size = 16296\n",
    "embedding_size = 50\n",
    "epochs_num = 50\n",
    "batch_size = 401\n",
    "margin = 1\n",
    "learning_rate = 1e-1\n",
    "\n",
    "#定义模型\n",
    "def l1_energy(batch):\n",
    "    return tf.reduce_sum(tf.abs(batch[:, 0, :] + batch[:, 1, :] - batch[:, 2, :]), 1)\n",
    "\n",
    "embedding_table = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
    "\n",
    "pos_triples_batch = tf.placeholder(tf.int32, shape=[batch_size, 3])\n",
    "neg_triples_batch = tf.placeholder(tf.int32, shape=[batch_size, 3])\n",
    "predict_triples_batch = tf.placeholder(tf.int32,shape=[1,1])\n",
    "\n",
    "pos_embed_batch = tf.nn.embedding_lookup(embedding_table, pos_triples_batch)\n",
    "neg_embed_batch = tf.nn.embedding_lookup(embedding_table, neg_triples_batch)\n",
    "predict_embed_batch = tf.nn.embedding_lookup(embedding_table, predict_triples_batch)\n",
    "\n",
    "p_loss = l1_energy(pos_embed_batch)\n",
    "n_loss = l1_energy(neg_embed_batch)\n",
    "\n",
    "loss = tf.reduce_sum(tf.nn.relu(margin + p_loss - n_loss))\n",
    "optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "for step in range(epochs_num):\n",
    "    time_start = time.time() #开始计时\n",
    "    loss_total = 0\n",
    "    print('start:',keep)\n",
    "    \n",
    "    for i in range(len(golden_data)//batch_size + 1):\n",
    "        \n",
    "        golden_batch,invalid_batch = nextbatch(batchsize=batch_size)    \n",
    "        feed_dict = {pos_triples_batch: golden_batch, neg_triples_batch: invalid_batch}\n",
    "        loss_val, _ = sess.run([loss, optimizer], feed_dict=feed_dict)\n",
    "        loss_total += loss_val\n",
    "        \n",
    "    print(\"\\rstep %d, loss_val %f\" % (step, loss_total/(len(golden_data)//batch_size + 1)))\n",
    "    time_end = time.time()    #结束计时\n",
    "    sum_t= time_end - time_start   #运行所花时间\n",
    "    print('time cost', sum_t, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_embedding_index = []\n",
    "\n",
    "for entity in list(reverse_index):\n",
    "    encode = reverse_index[entity]\n",
    "    embedding = sess.run([predict_embed_batch], feed_dict= {predict_triples_batch: [[encode]]})[0][0][0]\n",
    "    total_embedding_index.append([entity,embedding])\n",
    "    \n",
    "TransE_index = dict(total_embedding_index)\n",
    "# print(TransE_index['/m/035yzw'])\n",
    "\n",
    "entity_embedding_index = []\n",
    "for entity in list(total_entities):\n",
    "    encode = reverse_index[entity]\n",
    "    embedding = sess.run([predict_embed_batch], feed_dict= {predict_triples_batch: [[encode]]})[0][0][0]\n",
    "    entity_embedding_index.append([entity,embedding])\n",
    "    \n",
    "TransE_entity_index = dict(entity_embedding_index)\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "链接预测评测代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读入训练数据\n",
    "# 先用hit@10标准测试一下\n",
    "valid_file = open('链路预测数据集/FB15k/freebase_mtr100_mte100-valid.txt')\n",
    "valid_data = valid_file.read()\n",
    "valid_triples = valid_data.split('\\n')[:50000]\n",
    "\n",
    "#生成验证集实体和关系集合\n",
    "valid_head_entities = list([triple.split('\\t')[0] for triple in valid_triples])\n",
    "valid_relations =  list([triple.split('\\t')[1] for triple in valid_triples])\n",
    "valid_tail_entities =  list([triple.split('\\t')[2] for triple in valid_triples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('开始预测')\n",
    "import numpy as np\n",
    "# import numpy_gpu as np\n",
    "# import minpy.numpy as np\n",
    "\n",
    "hit10 = 0\n",
    "hit50 = 0\n",
    "hit1000 = 0\n",
    "num = 0\n",
    "\n",
    "def takeFirst(elem):\n",
    "    return elem[0]\n",
    "\n",
    "# for head,relation,tail in zip(head_entities[:10],relations[:10],tail_entities[:10]):\n",
    "for head,relation,tail in zip(valid_head_entities,valid_relations,valid_tail_entities):\n",
    "    num = num+1\n",
    "    \n",
    "    print(head,relation,tail)\n",
    "    #　头实体嵌入\n",
    "    valid_head_embedding = TransE_index[head]\n",
    "    #　关系的嵌入\n",
    "    valid_relation_embedding = TransE_index[relation]\n",
    "    #　头尾实体加和\n",
    "    head_and_relation = valid_head_embedding + valid_relation_embedding\n",
    "    # 预测一个尾实体\n",
    "    predict_tail_embedding = head_and_relation\n",
    "    \n",
    "    #　与所有的实体计算距离\n",
    "    distance_list = []\n",
    "    \n",
    "    # 遍历TransE嵌入中的所有实体\n",
    "    for index,entity in enumerate(list(TransE_entity_index)):\n",
    "        \n",
    "        #　与TransE中的所有实体进行比较\n",
    "        embedding = TransE_index[entity]\n",
    "        \n",
    "        # 二阶距离\n",
    "        distance = np.linalg.norm(predict_tail_embedding - embedding)\n",
    "#         distance = np.sqrt(np.sum(np.square(predict_tail_embedding - embedding)))\n",
    "        distance_list.append([distance,entity])\n",
    "\n",
    "    # 对所有距离进行排序\n",
    "    distance_list.sort(key=takeFirst)\n",
    "\n",
    "    print(distance_list[0])\n",
    "\n",
    "    predict_1000_list = list([entity[1] for entity in distance_list])[:1000]\n",
    "    predict_50_list = list([entity[1] for entity in distance_list])[:50]\n",
    "    predict_10_list = list([entity[1] for entity in distance_list])[:10]\n",
    "\n",
    "    if tail in predict_1000_list:\n",
    "        hit1000 = hit1000+1\n",
    "#         print('hit@1000yes')\n",
    "#     else:\n",
    "#         print('hit@1000no')\n",
    "        \n",
    "    if tail in predict_50_list:\n",
    "        hit50 = hit50+1\n",
    "#         print('hit@50yes')\n",
    "#     else:\n",
    "#         print('hit@50no')\n",
    "        \n",
    "    if tail in predict_10_list:\n",
    "        hit10 = hit10 +1\n",
    "#         print('hit@10yes')\n",
    "#     else:\n",
    "#         print('hit@10no')\n",
    "        \n",
    "    print('hit',hit10/num,hit50/num,hit1000/num,num)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
